<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" type="image/png" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Xzy"><meta name="keywords" content=""><title>DETR 论文阅读 - ZiYang&#39;s Blog</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.1.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"xcraft.tech",root:"/",version:"1.8.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},copy_btn:!0,image_zoom:{enable:!0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}}}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>ZiYang-Xie's Blog</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/img/bg_in.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="DETR 论文阅读"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-01-22 13:47" pubdate>2021年1月22日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 15 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">DETR 论文阅读</h1><div class="markdown-body"><h1 id="End-to-End-Object-Detection-with-Transformers"><a href="#End-to-End-Object-Detection-with-Transformers" class="headerlink" title="End-to-End Object Detection with Transformers"></a>End-to-End Object Detection with Transformers</h1><p><em>关键词： 目标检测， Transformer，End-to-End</em></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>​ Detr 这篇文章抛弃了传统 Fast-RCNN 基于 ROI 的目标检测方式，使用了Transformer以及其提出的 bipartite matching 做到位置无关和生成唯一的目标检测框，以此省去了 Anchor 和 NMS。以非常简单的架构达到媲美甚至超越 Fast-RCNN 的准确率，在能够做目标检测的同时，该模型还有较好的迁移能力，在原论文中通过 Transformer 的 Attention 机制实现了全景分割。</p><h2 id="结构分析"><a href="#结构分析" class="headerlink" title="结构分析"></a>结构分析</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmwgc8ubqoj310s07t435.jpg" srcset="/img/loading.gif" alt="网络结构图"></p><p>​ DETR 的网络结构很简单，分为三个部分，第一部分是一个传统 CNN 用于提取图片特征到更高维度，第二部分一个Transformer 的 Encoder 和 Decoder 来提取 Bounding Box，最后使用 Bipartite matching loss 来训练网络。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmwghk322pj30m8069adc.jpg" srcset="/img/loading.gif" alt="结构细节"></p><p>​ 更加细致的划分可划分为 CNN backbone 部分，Transformer 中的 Encoder 和 Decoder 部分，预测前馈网络 (FFN) 部分。接下来会详细讲解。</p><h3 id="CNN-部分"><a href="#CNN-部分" class="headerlink" title="CNN 部分"></a>CNN 部分</h3><p>​ DETR 的第一部分用了一个 CNN backbone 将 $x_{img} \in R^{3 \times H_0 \times W_0}$ (3 的 RGB 深度) 转换为 $f \in R^{C \times H \times W}$ 的特征层。论文中用了 $C = 2048, H = \frac{H_0}{32}, W = \frac{W_0}{32}$</p><h3 id="Transformer-部分"><a href="#Transformer-部分" class="headerlink" title="Transformer 部分"></a>Transformer 部分</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>​ 先用 1x1 的卷积核将纬度从 C 降到 d，获得一个新的特征图 $R^{ d \times H \times W}$ 。由于 Encoder 需要一个序列，我们要将特征图拉平成为 $d \times HW$ 的向量，输入到 encoder 中，每一个 encoder 都是同样的结构，由一个多头注意力模块和一个前馈网络（FFN）组成。不像RNN，transformer 的输入是顺序无关的，于是我们也学 NLP 对每一个注意力层加一个位置编码 （position encodings）。将状态编码和之前拉平的特征图向量相加之后喂入 encoder 中。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmwioerjhbj30ea0ghgq4.jpg" srcset="/img/loading.gif" style="zoom:80%"></p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>​ decoder 的输入有两个，一个是 Object Queries 另一个是刚刚 Encoder 的输出，结构和传统 Transformer 差不多。比较有意思的是这个输入的 Object Queries，由于 Transformer 是 fixed size，如果我们需要 N 个 Bounding Box 那么我们就需要 N 个输入，同时这个 Object Queries 顺便充当了 decoder 的 position encodings，是通过学习得来的。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmwi2hocbcj30eh0gb421.jpg" srcset="/img/loading.gif" style="zoom:80%"></p><p>​ encoder 的输出直接喂到的是 Encoder-Decoder Attention 层，这 N 个位置嵌入要先通过自注意力层才获得 encoder 的信息。作者将这 N 个 序列最后生成的 Bounding Box 拿出来可视化，结果非常 Amazing 啊。</p><p>​ <img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmwinw5iwbj30wn075ale.jpg" srcset="/img/loading.gif" alt></p><p>​ 图中的不同颜色的点代表不同大小形态的 Bounding Box，绿色代表较小的 Bounding Box，紫色代较大的 Bounding Box，红色代表大的水平的 boxes，蓝色代表大的竖直的 boxes。 对于每一个输入序列，其都有所侧重，有的侧重与左侧的小 Bounding Box 有的侧重于中间大的Bounding Box。</p><h3 id="Bipartite-matching-loss"><a href="#Bipartite-matching-loss" class="headerlink" title="Bipartite matching loss"></a>Bipartite matching loss</h3><p>​ 之前检测器往往通过anchor和groundtruth的IOU来确定正负样本，而DETR使用了bipartite matching loss 来确定正负样本。</p><p>​ <img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmzoqacu30j307d01waa0.jpg" srcset="/img/loading.gif" alt="bipartite matching loss"></p><p>​ 其中 $L_{match}$ 是 ground true 和预测 bounding boxes 之间的 pair-wise matching cost （一一配对 penalty）</p><p>​ <img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmzoq11i5jj303405hmxa.jpg" srcset="/img/loading.gif" alt="bipartite matching"></p><p>​ 通过一一配对，就不需要再采取传统的 NMS 了，因为一个bounding box 只能和一个 ground true 进行匹配，必然会引入 loss。算法实现采用的是<strong>匈牙利算法</strong>，这个后期我会写在博客上。</p><h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><p>​ 最后得出结果的网络，是一个三层的感知机，activation function 用的是 ReLU，以及一个线性映射层。输出的是每一个 bounding box 的中心坐标，以及它的宽高 $(x,y,w,h)$ 以及物体的分类。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​ 原文在 COCO 2017 数据集上做的实验，模型虽然简单但却消耗了大量的训练时间 （Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU），最后效果媲美甚至超过了经过良好调教的 Fast-RCNN 类的人工 head + anchor 的模式。</p><p>​ <img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmzpkgtz3xj30gj06n75k.jpg" srcset="/img/loading.gif" alt="实验结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​ 这篇文章还是相当惊艳的，一直以来不管是 anchor based 还是 anchor free 的目标检测方法都难以脱离人工定义 anchor 的过程，而本篇文章通过使用 Transformer + positional encoding 达到甚至超越了传统方法的性能，里面特别是 positional encodings 的 object queries 非常耐人寻味，这些 queries 学到的真的只是 positional encoding吗？这个 queries 是否有可能是 tasks 无关的？如果是能不能通过预训练的方式来提高训练速度，这些问题都是后期值得探索的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko: “End-to-End Object Detection with Transformers”, 2020; <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2005.12872">arXiv:2005.12872</a></p><p>[2] 如何评价FAIR的新论文DETR？ <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/397692959/answer/1258046044">https://www.zhihu.com/question/397692959/answer/1258046044</a></p><p>[3] 如何看待End-to-End Object Detection with Transformers? <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/397624847/answer/1250143331">https://www.zhihu.com/question/397624847/answer/1250143331</a></p><p>[4] 详解Transformer （Attention Is All You Need）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">https://zhuanlan.zhihu.com/p/48508221</a></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/Research/">Research</a> <a class="hover-with-bg" href="/categories/Research/Object-Detection/">Object Detection</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/Transformer/">Transformer</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/2021/03/08/ICS/ICS_Normal/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">排序算法及编译器优化效果对比</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/01/06/ICS/ICS_PJ/"><span class="hidden-mobile">ICS_PJ Y86 Extended CPU</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.waitElementVisible("gitalk-container",(function(){Fluid.utils.createCssLink("/css/gitalk.css"),Fluid.utils.createScript("https://cdn.staticfile.org/gitalk/1.7.0/gitalk.min.js",(function(){new Gitalk({clientID:"bb132e59582b2e328abc",clientSecret:"884bfc0ac692d040744d5e4b81ffd6f1aa95cbc0",repo:"CsBlog",owner:"ZiYang-xie",admin:["ZiYang-xie"],id:"b9c12edc443a34f5798c1bb6de0dda2e",language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",createIssueManually:!0,distractionFreeMode:!1}).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><p>Fight For The Happiness of Humanity</p><div><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/CsBlog/js/duration.js"></script></div></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">沪ICP备20009486号-1</a></div></footer><script src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:200}),NProgress.start(),document.addEventListener("DOMContentLoaded",(function(){window.NProgress&&window.NProgress.inc()})),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.5.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/js/bootstrap.min.js"></script><script src="/js/debouncer.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/lazyload.js"></script><script src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js"></script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="https://cdn.staticfile.org/anchor-js/4.3.0/anchor.min.js"></script><script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/js/local-search.js"></script><script>document.querySelector("#local-search-input").onclick=function(){searchFunc("/local-search.xml","local-search-input","local-search-result"),this.onclick=null}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const n=!!t.type.match(/; *mode=display/),o=new e.options.MathItem(t.textContent,e.inputJax[0],n),a=document.createTextNode("");t.parentNode.replaceChild(a,t),o.start={node:a,delim:"",n:0},o.end={node:a,delim:"",n:0},e.math.push(o)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.staticfile.org/mathjax/3.1.2/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>