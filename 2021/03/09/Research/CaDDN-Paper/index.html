<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/CsBlog/img/favicon.png"><link rel="icon" type="image/png" href="/CsBlog/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Xzy"><meta name="keywords" content=""><title>CaDDN论文阅读 - ZiYang&#39;s Blog</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/CsBlog/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/CsBlog/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"ziyang-xie.github.io",root:"/CsBlog/",version:"1.8.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},copy_btn:!0,image_zoom:{enable:!0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}}}</script><script src="/CsBlog/js/utils.js"></script><script src="/CsBlog/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/CsBlog/">&nbsp;<strong>ZiYang-Xie's Blog</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/CsBlog/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/CsBlog/img/bg_in.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CaDDN论文阅读"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-03-09 20:20" pubdate>2021年3月9日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 17 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CaDDN论文阅读</h1><div class="markdown-body"><h1 id="Categorical-Depth-Distribution-Network-for-Monocular-3D-Object-Detection"><a href="#Categorical-Depth-Distribution-Network-for-Monocular-3D-Object-Detection" class="headerlink" title="Categorical Depth Distribution Network for Monocular 3D Object Detection"></a><strong>Categorical Depth Distribution Network for Monocular 3D Object Detection</strong></h1><p><strong>关键词： 单目3d检测、绝对深度分配网络</strong></p><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.01100.pdf">https://arxiv.org/pdf/2103.01100.pdf</a></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1godx97xntnj30bl08p0wn.jpg" srcset="/CsBlog/img/loading.gif" alt></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​ 单目3D检测的难点一直在于对深度的预测，实例从3D空间被映射到2D平面的图像上丢失了深度信息，对深度的处理一直是单目3D目标检测研究的重点方向，目前主流的方法主要分为三类。</p><p><strong>1、直接检测 (Direct Methods)</strong></p><p>​ 直接检测方法没有显式的对深度进行学习，比较有代表性的是关键点检测方法，通过关键点结合几何特征来帮助3D-Bbox的检测，好处是简单直接且高效，但这类方法由于没有显式的学习深度信息，往往导致深度预测的结果不尽理想。</p><p><strong>2、基于深度 (Depth-Based Methods)</strong></p><p>​ 基于深度的方法通常先会通过一个单目深度预测分支来得到一张深度图作为网络的输入从而辅助对深度的检测，由于有了深度信息其可被转换成点云来处理（可以用上3d检测的方法)。但由于其深度和目标检测分离训练的结构，导致其可能会丢失一些隐含的信息。</p><p><strong>3、基于网格 (Grid-Based Methods)</strong></p><p>​ 基于网格的方法避免了对深度的直接预测，而是通过预测一个 BEV grid 的表达来作为3D检测网络的输入，OFT[1]提出了一种体素网格，通过把体素投影到图像平面上进而采样图像特征将其转换成BEV的形式。但这也会导致大量体素和特征的重叠从而降低检测的准确性。</p><p>​ <strong>CaDDN</strong> 网络对上面三种情况的优点进行结合，整体网络结构是同时训练了深度预测和3D检测（jointly）以期待其能够解决方法2中的问题，同时利用也将图像平面转换成了BEV的形式来提高检测的准确性。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1godxz73f1jj30nm0dntcy.jpg" srcset="/CsBlog/img/loading.gif" alt="网络结构"></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="Frustum-Feature-Network"><a href="#Frustum-Feature-Network" class="headerlink" title="Frustum Feature Network"></a>Frustum Feature Network</h3><ul><li><strong>Depth Distribution Network</strong></li></ul><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1godzoe00t7j30b908dabe.jpg" srcset="/CsBlog/img/loading.gif" alt></p><p>对于特征提取网络，其输出形似一个截角锥形，因而作者称其为Frustum feature Network，其输入是原始图像 $I \in R^{W_1\times H_1\times 3}$ 输出 $G \in R^{W_F\times H_F\times D \times C}$ 其中 W和H是特征的宽高，D 是深度桶，用以深度预测，C 是特征的维度。图像特征被用以在每个像素上预测绝对的深度分布 $D \in R^{W_F\times H_F\times D}$ 网络对每个像素预测其落入某一深度桶（深度离散化）的概率，总共D个。</p><p>（这一网络其是从 DeepLabV3[2] 上魔改过来的。）</p><ul><li>Image Channel Reduce</li></ul><p>同时在分配深度桶的同时，网络另一分支用1x1卷积 + BN + ReLU 把特征的维数从256降到了64。</p><p>​ 经过这两个分支后，将预测出来的深度桶和特征像素做外积得到了带有深度信息的特征图（$G(u,v) = D(u,v) \otimes F(u,v)$）且由于特征桶的结构，具有较高的容错性。称 G 为 frustum features</p><h3 id="Frustum-to-Voxel-Trans"><a href="#Frustum-to-Voxel-Trans" class="headerlink" title="Frustum to Voxel Trans"></a>Frustum to Voxel Trans</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1godzokeng2j30b907ujsr.jpg" srcset="/CsBlog/img/loading.gif" alt></p><p>​ G 之后将被转换成体素的形式，这里的问题是在第分辨率的frustum features进行高分辨率的体素采样会导致过采样，导致出现大量相同的体素特征，浪费算力不说还降低预测准确性。所以这边作者直接把降采样前的特征层拉了过来，来保证不会出现上述问题。</p><h3 id="Voxel-Collapse-to-BEV-gt-Detection"><a href="#Voxel-Collapse-to-BEV-gt-Detection" class="headerlink" title="Voxel Collapse to BEV -&gt; Detection"></a>Voxel Collapse to BEV -&gt; Detection</h3><p>​ 由于BEV在保证同等检测效果的情况下能够节省计算资源，作者将体素的 Z 轴和 channels 直接连接起来，继续用1x1卷积 + BN + ReLU 将体素块压缩成 BEV的形式。然后接着在 BEV 特征块上连接检测头进行 3D目标检测。</p><h2 id="深度离散化"><a href="#深度离散化" class="headerlink" title="深度离散化"></a>深度离散化</h2><p>​ 本文的主要亮点就是其对于深度的处理，其对每个像素位置分配了离散化的深度桶，预测其属于某一深度的概率。这里的深度离散化作者使用的是 LID (Linear-increasing discretization)[3] 因为其对不同深度之间提供了最为平衡的预测概率。对于检测任务的深度我们最需要的是检测目标的深度信息，而更少去在意背景点的深度。</p><p>​<script type="math/tex">d_c = d_min + \frac{d_{max}-d_{min}}{D(D+1)}\cdot{d_i}{(d_1+1)}</script></p><p>​ <em>（其中 $d<em>c$ 是连续深度值，[$d</em>{min},d_{max}$]是离散化的上下界，D是深度桶的数量，$d_i$ 是下标）</em></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1goe03wlr9sj30dn0bd0tp.jpg" srcset="/CsBlog/img/loading.gif" alt></p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1goe0ckxhf6j30r10fgn17.jpg" srcset="/CsBlog/img/loading.gif" alt="KITTI实验效果"></p><p>​ 可以看到其在车辆和新人检测上都打破了目前最好的检测方法，对骑行者的检测不如 MonoPSR 但也大大超过了其余检测模型。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1goe0gh8vyhj30dw07jdgw.jpg" srcset="/CsBlog/img/loading.gif" alt></p><p>这张表可以看出其将深度预测和特征融合所得到的 frustum features 确实有助于提升检测效果。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>​ 个人觉得这篇paper的主要成功点在于其对于单目深度预测的创新解决方法，不同于传统的深度预测模型，提出了离散化深度将每个像素的深度概率离散化分配在不同的深度桶中，避免了网络过度依赖于深度的准确检测。但同时其也没有从根本上解决深度预测的问题，同时其中间对于特征图进行体素化投影再转为 BEV 的过程较为复杂，虽然其把降采样前的特征层拿来转成体素形式但是仍然还是无法避免会有损失。</p><p>​ 个人觉得可以借鉴该模型的深度预测方法，应用在现有的模型上或者对该模型的网络结构进行简化，可能会带来更好的效果。</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho graphic feature transform for monocular 3D object detection.<em>BMVC</em>, 2018</p><p>[2] Liang-Chieh Chen, George Papandreou, Florian Schroff, andHartwig Adam. Rethinking atrous convolution for semantic image segmentation. <em>arXiv preprint</em>, 2017</p><p>[3] Yunlei Tang, Sebastian Dorn, and Chiragkumar Savani. Center3d: Center-based monocular 3d object detection with joint depth understanding. <em>arXiv preprint</em>, 2020</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/CsBlog/categories/Research/">Research</a> <a class="hover-with-bg" href="/CsBlog/categories/Research/Object-Detection/">Object Detection</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/CsBlog/tags/Monocular-OD/">Monocular OD</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/CsBlog/2021/03/22/Readelf/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">ELF头简介</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/CsBlog/2021/03/08/ICS/ICS_Normal/"><span class="hidden-mobile">排序算法及编译器优化效果对比</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.waitElementVisible("gitalk-container",(function(){Fluid.utils.createCssLink("/CsBlog/css/gitalk.css"),Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.0/gitalk.min.js",(function(){new Gitalk({clientID:"bb132e59582b2e328abc",clientSecret:"884bfc0ac692d040744d5e4b81ffd6f1aa95cbc0",repo:"CsBlog",owner:"ZiYang-xie",admin:["ZiYang-xie"],id:"062505a1802201ffdca84d9258ce5c15",language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",createIssueManually:!0,distractionFreeMode:!1}).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><p>Fight For The Happiness of Humanity</p><div><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/CsBlog/js/duration.js"></script></div></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">沪ICP备20009486号-1</a></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:200}),NProgress.start(),document.addEventListener("DOMContentLoaded",(function(){window.NProgress&&window.NProgress.inc()})),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.3/js/bootstrap.min.js"></script><script src="/CsBlog/js/debouncer.js"></script><script src="/CsBlog/js/events.js"></script><script src="/CsBlog/js/plugins.js"></script><script src="/CsBlog/js/lazyload.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.0/tocbot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.3.0/anchor.min.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/typed.js/2.0.11/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/CsBlog/js/local-search.js"></script><script>document.querySelector("#local-search-input").onclick=function(){searchFunc("/CsBlog/local-search.xml","local-search-input","local-search-result"),this.onclick=null}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const n=!!t.type.match(/; *mode=display/),o=new e.options.MathItem(t.textContent,e.inputJax[0],n),a=document.createTextNode("");t.parentNode.replaceChild(a,t),o.start={node:a,delim:"",n:0},o.end={node:a,delim:"",n:0},e.math.push(o)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.2/es5/tex-svg.js"></script><script src="/CsBlog/js/boot.js"></script></body></html>