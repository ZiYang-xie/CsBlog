<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/CsBlog/img/favicon.png"><link rel="icon" type="image/png" href="/CsBlog/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Xzy"><meta name="keywords" content=""><title>CS231n Training Neural Networks I 05 - ZiYang&#39;s Blog</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/CsBlog/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/CsBlog/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"ziyang-xie.github.io",root:"/CsBlog/",version:"1.8.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},copy_btn:!0,image_zoom:{enable:!0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}}}</script><script src="/CsBlog/js/utils.js"></script><script src="/CsBlog/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/CsBlog/">&nbsp;<strong>ZiYang-Xie's Blog</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/CsBlog/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/CsBlog/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/CsBlog/img/bg_in.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CS231n Training Neural Networks I 05"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-13 21:09" pubdate>2020年11月13日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 478 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 9 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CS231n Training Neural Networks I 05</h1><div class="markdown-body"><h3 id="OverView"><a href="#OverView" class="headerlink" title="OverView"></a>OverView</h3><ol><li>One time setup</li><li>Training dynamics</li><li>Evaluation</li></ol><h3 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h3><ul><li>Activation Functions</li><li>Data Preprocessing</li><li>Weight initialization</li><li>Batch Normalization</li><li>Babysitting the Learning Process</li><li>Hyperparameter Optimization</li></ul><hr><h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9uofs.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9uIYj.png" srcset="/CsBlog/img/loading.gif" alt></p><h4 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h4><script type="math/tex;mode=display">\sigma(x) = 1/(1+e^{-x})</script><ul><li>Squashes numbers to range [0,1]</li><li>Historically popular “firing rate” of a neuron</li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9uhTg.png" srcset="/CsBlog/img/loading.gif" alt></p><p><strong>3 Problems</strong></p><ol><li>Saturated the neural may kill the gradient.</li></ol><p><img src="https://s3.ax1x.com/2020/11/13/D9u5kQ.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>x is a very negative and very positive val, its gradient will be killed to zero</p></blockquote><ol><li>Sigmoid outputs are not zero-centered</li></ol><p><img src="https://s3.ax1x.com/2020/11/13/D9u7pn.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>For the sign of x and gradient is always the same, it gonna behaves like is above pic.</p><p>thats why we need zero-mean data, to optimize the w just through the zig zag path.</p></blockquote><ol><li>exp() is a bit compute expensive</li></ol><hr><h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><ul><li>Squashes numbers to range [-1, 1]</li><li>zero centered (nice)</li><li>still kills gradients when saturated</li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9uHlq.png" srcset="/CsBlog/img/loading.gif" alt></p><hr><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><script type="math/tex;mode=display">f(x) = max(0, x)</script><p><img src="https://s3.ax1x.com/2020/11/13/D9ub60.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>Does not saturate (in + region)</li><li>Very computationally efficient</li><li>Converges much faster than sigmoid/tanh in practice</li><li>Actually more biologically plausible than sigmoid</li></ul><p><strong>Problems</strong></p><ol><li>Not zero-centered output</li><li>An annoyance:</li><li>when x &lt;= 0 the gradient is slashed to zero (kill half the gradient)</li></ol><p><img src="https://s3.ax1x.com/2020/11/13/D9uX0U.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>Bad Init</li><li>Learning rate too high</li></ul><blockquote><p>people like to initialize ReLU neurons with slightly positive biases (eg 0.01), to increase the possibility that being activated.</p></blockquote><hr><h4 id="Leaky-ReLu"><a href="#Leaky-ReLu" class="headerlink" title="Leaky ReLu"></a>Leaky ReLu</h4><script type="math/tex;mode=display">f(x) = max(0.01x,x)</script><p><img src="https://s3.ax1x.com/2020/11/13/D9uqXV.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>Does not saturated</li><li>Computationally efficient</li><li>Converges much faster …</li><li><strong>will not die</strong></li></ul><p>or <strong>Para Rectifier ReLu</strong></p><script type="math/tex;mode=display">f(x) = max(\alpha{x},x)</script><hr><h4 id="Exponential-Linear-Units-ELU"><a href="#Exponential-Linear-Units-ELU" class="headerlink" title="Exponential Linear Units (ELU)"></a>Exponential Linear Units (ELU)</h4><p><img src="https://s3.ax1x.com/2020/11/13/D9uOmT.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>All benefits of ReLU</li><li>Closer to zero mean outputs</li><li>Negative saturation regime adds some robustness to noise</li></ul><p><em>While it requires exp()</em></p><hr><h4 id="Maxout-“Neuron”"><a href="#Maxout-“Neuron”" class="headerlink" title="Maxout “Neuron”"></a>Maxout “Neuron”</h4><p><img src="https://s3.ax1x.com/2020/11/13/D9uj7F.png" srcset="/CsBlog/img/loading.gif" alt></p><hr><h4 id="In-practice"><a href="#In-practice" class="headerlink" title="In practice"></a>In practice</h4><ul><li>Use ReLU. (zbe careful with learning rates)</li><li>Try Leaky <em>ReLU / Maxout / ELU</em></li><li>Don’t use sigmoid</li></ul><hr><h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><h4 id="Step1-Preprocess-the-data"><a href="#Step1-Preprocess-the-data" class="headerlink" title="Step1: Preprocess the data"></a>Step1: Preprocess the data</h4><p><img src="https://s3.ax1x.com/2020/11/13/D9uxk4.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>In CV we usually don’t normalize the data.</li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9KSh9.png" srcset="/CsBlog/img/loading.gif" alt></p><p>Practice above make the data zero-mean, but only the first layer, and that’s why we need the activation func tobe zero-mean.</p><hr><h3 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h3><h4 id="First-idea-Small-random-numbers"><a href="#First-idea-Small-random-numbers" class="headerlink" title="First idea: Small random numbers"></a>First idea: Small random numbers</h4><pre><code class="hljs python">W = <span class="hljs-number">0.01</span>* np.random.randn(D,H)</code></pre><blockquote><p>Works Okay for small, but have problems in big one.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9KC11.png" srcset="/CsBlog/img/loading.gif" alt></p><h4 id="How-about-making-Weight-big"><a href="#How-about-making-Weight-big" class="headerlink" title="How about making Weight big?"></a>How about making Weight big?</h4><p><img src="https://s3.ax1x.com/2020/11/13/D9KP6x.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>it gonna saturated the regime to be either very possitive or very negative input of tanh, and comes out near zero gradients. The weight will not be updated.</p></blockquote><h4 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h4><p><em>Woo my initialzation? haha</em></p><p><img src="https://s3.ax1x.com/2020/11/13/D9KiX6.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>ensure we are at the active region of tanh</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9KA0O.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>Can be addressed by add an extra /2， to ensure the neural won’t die in ReLU</p></blockquote><hr><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9KknK.png" srcset="/CsBlog/img/loading.gif" alt></p><blockquote><p>To regulize the input tobe unit gaussian.</p></blockquote><p><em>I have no idea about it. What is unit gaussian?</em></p><p><img src="https://s3.ax1x.com/2020/11/13/D9KetH.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9KE7D.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9KZAe.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9Kmhd.png" srcset="/CsBlog/img/loading.gif" alt></p><hr><h3 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h3><ul><li><ol><li>Preprocess data</li></ol></li><li><ol><li>Choose the architecture:</li></ol></li><li><ol><li>Double check that the loss is reasonable</li></ol></li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9KK1I.png" srcset="/CsBlog/img/loading.gif" alt></p><h4 id="The-Learning-Rate"><a href="#The-Learning-Rate" class="headerlink" title="The Learning Rate"></a>The Learning Rate</h4><ul><li>Very small learning rate 1e-6<blockquote><p>litter help</p></blockquote></li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9K3B8.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>Very great learning rate 1e6<blockquote><p>go extreme</p></blockquote></li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9KQjP.png" srcset="/CsBlog/img/loading.gif" alt></p><ul><li>A Rough Range</li></ul><script type="math/tex;mode=display">1\times{10}^{-3} \to 1\times{10}^{-5}</script><hr><h3 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h3><ul><li><p>Cross-validation strategy<br><em>coarse -&gt; fine</em></p></li><li><p>Random Sample</p></li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9Ku9A.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9KMct.png" srcset="/CsBlog/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9K1nf.png" srcset="/CsBlog/img/loading.gif" alt></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/CsBlog/categories/CS231n/">CS231n</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/CsBlog/tags/CV/">CV</a> <a class="hover-with-bg" href="/CsBlog/tags/Neural-Network/">Neural Network</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/CsBlog/2020/11/13/CS231n/CS231n-06-Training-Neural-Networks-II/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">CS231n Training Neural Networks II 06</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/CsBlog/2020/11/11/CS231n/CS231n-04-Convolutional-Neural-Networks/"><span class="hidden-mobile">CS231n Convolutional_Neural_Networks 04</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.waitElementVisible("gitalk-container",(function(){Fluid.utils.createCssLink("/CsBlog/css/gitalk.css"),Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.0/gitalk.min.js",(function(){new Gitalk({clientID:"bb132e59582b2e328abc",clientSecret:"884bfc0ac692d040744d5e4b81ffd6f1aa95cbc0",repo:"CsBlog",owner:"ZiYang-xie",admin:["ZiYang-xie"],id:"3fe68408bf25d35e33bf2f0d46a62b18",language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",createIssueManually:!0,distractionFreeMode:!1}).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><p>Fight For The Happiness of Humanity</p><div><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/CsBlog/js/duration.js"></script></div></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">沪ICP备20009486号-1</a></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:200}),NProgress.start(),document.addEventListener("DOMContentLoaded",(function(){window.NProgress&&window.NProgress.inc()})),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.3/js/bootstrap.min.js"></script><script src="/CsBlog/js/debouncer.js"></script><script src="/CsBlog/js/events.js"></script><script src="/CsBlog/js/plugins.js"></script><script src="/CsBlog/js/lazyload.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.12.0/tocbot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.3.0/anchor.min.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/typed.js/2.0.11/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/CsBlog/js/local-search.js"></script><script>document.querySelector("#local-search-input").onclick=function(){searchFunc("/CsBlog/local-search.xml","local-search-input","local-search-result"),this.onclick=null}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const n=!!t.type.match(/; *mode=display/),o=new e.options.MathItem(t.textContent,e.inputJax[0],n),a=document.createTextNode("");t.parentNode.replaceChild(a,t),o.start={node:a,delim:"",n:0},o.end={node:a,delim:"",n:0},e.math.push(o)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.2/es5/tex-svg.js"></script><script src="/CsBlog/js/boot.js"></script></body></html>