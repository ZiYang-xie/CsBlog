<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" type="image/png" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Xzy"><meta name="keywords" content=""><title>CS231n Training Neural Networks II 06 - Xavier&#39;s Blog</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.1.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"xcraft.tech",root:"/",version:"1.8.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},copy_btn:!0,image_zoom:{enable:!0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}}}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>Xavier’s Blog</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/img/bg_in.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CS231n Training Neural Networks II 06"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-13 21:11" pubdate>2020年11月13日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 528 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 10 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CS231n Training Neural Networks II 06</h1><div class="markdown-body"><h3 id="PreView"><a href="#PreView" class="headerlink" title="PreView"></a>PreView</h3><ul><li>Fancier optimization</li><li>Regularization</li><li>Transfer Learning</li></ul><hr><h3 id="Problem-with-SGD"><a href="#Problem-with-SGD" class="headerlink" title="Problem with SGD"></a>Problem with SGD</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9JOns.png" srcset="/img/loading.gif" alt></p><blockquote><p>The zig-zag path reveal the drawbacks of SGD</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9JbcQ.png" srcset="/img/loading.gif" alt></p><blockquote><p>Stuck in the local minima.</p></blockquote><ul><li>Saddle points much more common in high dimension.</li></ul><p><strong>Add an Momentum term may solve these problems</strong></p><p><img src="https://s3.ax1x.com/2020/11/13/D9JqXj.png" srcset="/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9JXBn.png" srcset="/img/loading.gif" alt></p><blockquote><p>Owing to the exsistance of momentum we can training more faster and overcome the problems mentioned before.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9JH1g.png" srcset="/img/loading.gif" alt></p><ul><li><strong>Nesterov Momentum</strong></li></ul><script type="math/tex;mode=display">v_{t+1} = \rho{v_t}-\alpha{\nabla{f(x_t+\rho{v_t})}}
\\
x_{t+1} = x_t + v_{t+1}</script><p><img src="https://s3.ax1x.com/2020/11/13/D9Jj7q.png" srcset="/img/loading.gif" alt></p><blockquote><p>some kind error correcting term of present v and the previous v</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9JzNV.png" srcset="/img/loading.gif" alt></p><hr><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><pre><code class="hljs python">grad_squared = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    dx = compute_gradient(x)
    grad_squared += dx * dx
    x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="hljs-number">1e-7</span>)</code></pre><blockquote><p>The basic idea about AdaGrad algorithm is that the step of dimention with smaller gradients will be divided by small vals and make it move faster, while greater one slower to avoid zig-zag behavior.</p><p>while the step will become smaller and smaller while you get closer to the minima, but in turn with higher risks to stuck in the local minima.</p></blockquote><hr><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9JxA0.png" srcset="/img/loading.gif" alt></p><blockquote><p>With a decay rate to make a smooth stop the reducing of steps.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9Y99U.png" srcset="/img/loading.gif" alt></p><hr><h3 id="Adam-almost"><a href="#Adam-almost" class="headerlink" title="Adam (almost)"></a>Adam (almost)</h3><pre><code class="hljs python">first_moment = <span class="hljs-number">0</span>
second_moment = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    dx = compute_gradient(x)
    first_moment = beta1 + first_moment + (<span class="hljs-number">1</span> - beta1) * dx
    <span class="hljs-comment"># Momentum</span>
    second_moment = beta2 * second_moment + (<span class="hljs-number">1</span> - beta2) * dx * dx
    <span class="hljs-comment"># AdaGrad / RMSProp</span>
    x -= learning_rate * first_moment / (np.sqrt(second_moment) + <span class="hljs-number">1e-7</span>)</code></pre><blockquote><p>It combine the two methods, but with a little bug of the first step, which gonna be super large.</p></blockquote><h3 id="Adam-full-form"><a href="#Adam-full-form" class="headerlink" title="Adam (full form)"></a>Adam (full form)</h3><pre><code class="hljs python">first_moment = <span class="hljs-number">0</span>
second_moment = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):
    dx = compute_gradient(x)
    first_moment = beta1 + first_moment + (<span class="hljs-number">1</span> - beta1) * dx
    <span class="hljs-comment"># Momentum</span>
    second_moment = beta2 * second_moment + (<span class="hljs-number">1</span> - beta2) * dx * dx
    <span class="hljs-comment"># AdaGrad / RMSProp</span>
    first_unbias = first_moment / (<span class="hljs-number">1</span> - beta1 ** t)
    second_unbias = second_moment / (<span class="hljs-number">1</span> - beta2 ** t)
    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + <span class="hljs-number">1e-7</span>)</code></pre><blockquote><p>Bias correction for the fact that first and second moment estimates start at zero</p></blockquote><ul><li>Great starting point</li></ul><ol><li>beta1 = 0.9</li><li>beta2 = 0.999</li><li>learning_rate = 1e-3 or 5e-4</li></ol><hr><h3 id="Decay-the-learning-rate-to-make-it-finer"><a href="#Decay-the-learning-rate-to-make-it-finer" class="headerlink" title="Decay the learning rate to make it finer"></a>Decay the learning rate to make it finer</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9YShT.png" srcset="/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9YPc4.png" srcset="/img/loading.gif" alt></p><hr><h3 id="little-bit-Fancier-Optimization"><a href="#little-bit-Fancier-Optimization" class="headerlink" title="little bit Fancier Optimization"></a>little bit Fancier Optimization</h3><p><img src="https://s3.ax1x.com/2020/11/13/D9YC3F.png" srcset="/img/loading.gif" alt></p><blockquote><p>First derivative optimization</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9YijJ.png" srcset="/img/loading.gif" alt></p><blockquote><p>Second derivative optimization, direct to the mini</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9Yku9.png" srcset="/img/loading.gif" alt></p><blockquote><p>Don’t need learning rate, but impractical for Hessian has O(N^2) elements and Inverting takes O(N^3)</p></blockquote><h4 id="Quasi-Newton-methods-BGFS"><a href="#Quasi-Newton-methods-BGFS" class="headerlink" title="Quasi - Newton methods (BGFS)"></a>Quasi - Newton methods (BGFS)</h4><p><img src="https://s3.ax1x.com/2020/11/13/D9YABR.png" srcset="/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9YEH1.png" srcset="/img/loading.gif" alt></p><hr><h3 id="In-Practice"><a href="#In-Practice" class="headerlink" title="In Practice:"></a>In Practice:</h3><ul><li>Using Adam</li><li>If full batch updates can be afforded, try out <strong>L-BFGS</strong></li></ul><hr><h3 id="Reduce-the-gap-between-train-and-unseen-data"><a href="#Reduce-the-gap-between-train-and-unseen-data" class="headerlink" title="Reduce the gap between train and unseen data"></a>Reduce the gap between train and unseen data</h3><h4 id="Model-Ensembles"><a href="#Model-Ensembles" class="headerlink" title="Model Ensembles"></a>Model Ensembles</h4><ol><li>Train multiple independent models</li><li>At test time average their results</li></ol><p>2% improvement maybe</p><p><img src="https://s3.ax1x.com/2020/11/13/D9YeN6.png" srcset="/img/loading.gif" alt></p><blockquote><p>Instead of using actual parameter vector, keep a moving average of the para vector and use that at test time</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    data_batch = dataset.sample_data_batch()
    loss = network.forward(data_batch)
    dx = network.backward()
    x += - learning_rate * dx
    x_test = <span class="hljs-number">0.995</span>*x_test + <span class="hljs-number">0.005</span>*x</code></pre><h4 id="Regularization-to-make-single-model-performs-better"><a href="#Regularization-to-make-single-model-performs-better" class="headerlink" title="Regularization to make single model performs better"></a>Regularization to make single model performs better</h4><ul><li>Dropout</li></ul><p><img src="https://s3.ax1x.com/2020/11/13/D9Ym4K.png" srcset="/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9YK3D.png" srcset="/img/loading.gif" alt></p><blockquote><p>Another interpretation is that you can percive each binary mask as a single model, so it just like dropout is training a large ensemble of models with shared paras.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9Yu9O.png" srcset="/img/loading.gif" alt></p><p><img src="https://s3.ax1x.com/2020/11/13/D9YMge.png" srcset="/img/loading.gif" alt></p><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><blockquote><p>Which can achieve the same effect as the Dropout, for it includes some noises.</p><h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><p>To introduce noise to make it performs better on unseen data.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9YQjH.png" srcset="/img/loading.gif" alt></p><h4 id="Stochastic-Depth"><a href="#Stochastic-Depth" class="headerlink" title="Stochastic Depth"></a>Stochastic Depth</h4><blockquote><p>Randomly drop layers during training.<br>Use the full networks during testing.</p></blockquote><hr><h3 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h3><blockquote><p>There is no need for huge amount of data.</p></blockquote><p><img src="https://s3.ax1x.com/2020/11/13/D9Y1ud.png" srcset="/img/loading.gif" alt></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/CS231n/">CS231n</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/CV/">CV</a> <a class="hover-with-bg" href="/tags/Neural-Network/">Neural Network</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/2020/11/15/ICS/ICS_Lab3/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">ICS-Lab4.2 各种链接姿势</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/11/13/CS231n/CS231n-05-Training-Neural-Networks-I/"><span class="hidden-mobile">CS231n Training Neural Networks I 05</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.waitElementVisible("gitalk-container",(function(){Fluid.utils.createCssLink("/css/gitalk.css"),Fluid.utils.createScript("https://cdn.staticfile.org/gitalk/1.7.0/gitalk.min.js",(function(){new Gitalk({clientID:"bb132e59582b2e328abc",clientSecret:"884bfc0ac692d040744d5e4b81ffd6f1aa95cbc0",repo:"CsBlog",owner:"ZiYang-xie",admin:["ZiYang-xie"],id:"248175eed9e26f4d29c01f91c6788295",language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",createIssueManually:!0,distractionFreeMode:!1}).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><p>Fight For The Happiness of Humanity</p><div><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/CsBlog/js/duration.js"></script></div></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">沪ICP备20009486号-1</a></div></footer><script src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:200}),NProgress.start(),document.addEventListener("DOMContentLoaded",(function(){window.NProgress&&window.NProgress.inc()})),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.5.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/js/bootstrap.min.js"></script><script src="/js/debouncer.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/lazyload.js"></script><script src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js"></script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="https://cdn.staticfile.org/anchor-js/4.3.0/anchor.min.js"></script><script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/js/local-search.js"></script><script>document.querySelector("#local-search-input").onclick=function(){searchFunc("/local-search.xml","local-search-input","local-search-result"),this.onclick=null}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const n=!!t.type.match(/; *mode=display/),o=new e.options.MathItem(t.textContent,e.inputJax[0],n),a=document.createTextNode("");t.parentNode.replaceChild(a,t),o.start={node:a,delim:"",n:0},o.end={node:a,delim:"",n:0},e.math.push(o)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.staticfile.org/mathjax/3.1.2/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>